"""
02b_expand_sku_from_category.py
- ì…ë ¥: cleaned_domain_sales.csv (ì—†ìœ¼ë©´ domain_sales.csv), sku_catalog.csv, (ì„ íƒ) promotions.csv
- ì²˜ë¦¬:
  * ì¹´í…Œê³ ë¦¬ í‘œì¤€í™” + í™”ì´íŠ¸ë¦¬ìŠ¤íŠ¸ í•„í„° (ì´ì¤‘ ë°©ì–´)
  * base_share Ã— (1+noise) Ã— (1+promo) í›„ ê·¸ë£¹ ì •ê·œí™”ë¡œ SKU ë¶„ë°°
  * sku_id í˜•ì‹ ê²€ì¦(ì˜ë¬¸ ëŒ€ë¬¸ì/ìˆ«ì/ëŒ€ì‹œ)
- ì¶œë ¥: domain_sales_sku.csv, sample_domain_sales_sku.csv
"""
from pathlib import Path
import pandas as pd
import numpy as np
import re

BASE = Path(__file__).resolve().parent
DOMAIN_CLEAN = BASE / "cleaned_domain_sales.csv"
DOMAIN_RAW   = BASE / "domain_sales.csv"
SKU_CAT      = BASE / "sku_catalog.csv"
PROMO        = BASE / "promotions.csv"   # optional: sku_id,target_date,boost

OUT_FULL   = BASE / "domain_sales_sku.csv"
OUT_SAMPLE = BASE / "sample_domain_sales_sku.csv"

# ===== Settings =====
NOISE_SCALE = 0.05          # Â±5% random noise
DEFAULT_PROMO_BOOST = 0.20  # when promotions.csv has no 'boost'
ID_PATTERN = r"^[A-Z0-9\-]+$"  # uppercase letters / digits / dash only
MISC_KEYS = {"", "ê¸°íƒ€", "ê¸°íƒ€ê°€ì „", "ê¸°íƒ€ê°€ì „ë¥˜", "ê¸°íƒ€ì œí’ˆ", "etc", "misc", "others", "other"}

def _normalize(x):
    s = x.sum()
    return x / s if s > 0 else x

def _norm_cat(x: str) -> str:
    if pd.isna(x):
        return ""
    x = str(x).strip()
    x = re.sub(r"\s+", "", x)        # ëª¨ë“  ê³µë°± ì œê±°
    x = x.replace("_", "").replace("-", "")
    return x.lower()

def _load_domain():
    domain_dtypes = {
        "warehouse_id": "Int64",
        "store_id": "Int64",
        "product_id": "Int64",
        "product_name": "string",
        "cat_top": "string",
        "cat_mid": "string",
        "cat_low": "string",
        "region": "string",
    }
    path = DOMAIN_CLEAN if DOMAIN_CLEAN.exists() else DOMAIN_RAW
    df = pd.read_csv(path, parse_dates=["target_date"], dtype=domain_dtypes, low_memory=False)
    print(f"ğŸ“¦ Loaded {path.name}: {len(df):,} rows")
    return df

def _load_catalog():
    # 1) ì „ì²´ë¥¼ ì¼ë‹¨ ë¬¸ìì—´ë¡œ ì•ˆì „í•˜ê²Œ ì½ëŠ”ë‹¤ (ì—´ ë°€ë¦¼/ë”°ì˜´í‘œ ì´ìŠˆ ë°©ì§€)
    sku = pd.read_csv(SKU_CAT, dtype=str, low_memory=False)

    # 2) í—¤ë”/ê³µë°± í‘œì¤€í™” (ì˜ˆìƒì¹˜ ëª»í•œ ê³µë°±/ëŒ€ì†Œë¬¸ì/ìˆ¨ì€ BOM ë°©ì§€)
    sku.columns = (
        sku.columns
          .astype(str)
          .str.replace("\ufeff", "", regex=False)  # BOM ì œê±°
          .str.strip()
    )

    # 3) í•„ìš”í•œ ì»¬ëŸ¼ì´ ì—†ë‹¤ë©´ ìƒì„±í•´ë‘ê¸° (NaN ë°©ì§€)
    for col in [
        "sku_id","sku_name_en","sku_name_ko","cat_low","brand","series","model_code",
        "size_inch","volume_l","capacity_text","energy_grade","price_tier",
        "msrp_krw","launch_date","warranty_months","case_pack","min_order_qty","eol_flag",
        "base_share"
    ]:
        if col not in sku.columns:
            sku[col] = pd.NA

    # 4) ìˆ«ì/ë‚ ì§œí˜•ë§Œ ê°œë³„ ìºìŠ¤íŒ… (ì—ëŸ¬ëŠ” NaNìœ¼ë¡œ í˜ë ¤ë³´ë‚¸ë‹¤)
    for col in ["msrp_krw","warranty_months","case_pack","min_order_qty","eol_flag"]:
        sku[col] = pd.to_numeric(sku[col], errors="coerce")

    sku["launch_date"] = pd.to_datetime(sku["launch_date"], errors="coerce")
    sku["base_share"]  = pd.to_numeric(sku["base_share"], errors="coerce")

    # 5) ë¬¸ìì—´ ì»¬ëŸ¼ì€ ì–‘ìª½ ê³µë°± ì •ë¦¬ (ì¡°ì¸ ì•ˆì •ì„± â†‘)
    for col in ["sku_id","sku_name_en","sku_name_ko","cat_low","brand","series","model_code",
                "size_inch","volume_l","capacity_text","energy_grade","price_tier"]:
        sku[col] = sku[col].astype("string").str.strip()

    return sku


def _load_promo():
    if PROMO.exists():
        p = pd.read_csv(PROMO, parse_dates=["target_date"])
        if "boost" not in p.columns:
            p["boost"] = DEFAULT_PROMO_BOOST
        return p
    return pd.DataFrame(columns=["sku_id", "target_date", "boost"])

def _validate_sku_ids(sku: pd.DataFrame):
    bad = sku[~sku["sku_id"].astype(str).str.match(ID_PATTERN, na=False)]
    if len(bad) > 0:
        ex = bad.head(5)["sku_id"].tolist()
        raise ValueError(f"sku_id í˜•ì‹ ì˜¤ë¥˜(ì˜ë¬¸ ëŒ€ë¬¸ì/ìˆ«ì/ëŒ€ì‹œë§Œ í—ˆìš©): ì˜ˆì‹œ {ex} ... ì „ì²´ {len(bad)}ê±´")

def main():
    # 0) load
    df  = _load_domain()
    sku = _load_catalog()

    # 1) í‘œì¤€í™”/í™”ì´íŠ¸ë¦¬ìŠ¤íŠ¸
    sku["cat_low_norm"] = sku["cat_low"].apply(_norm_cat)
    df["cat_low_norm"]  = df["cat_low"].apply(_norm_cat)

    allowed_norm = set(sku["cat_low_norm"].dropna().tolist())
    keep = df["cat_low_norm"].isin(allowed_norm) & (~df["cat_low_norm"].isin(MISC_KEYS))
    dropped = len(df) - keep.sum()
    if dropped:
        print(f"Dropped rows by whitelist in 02b: {dropped:,} / {len(df):,}")
    df = df[keep].copy()

    # 2) base_share í•©=1 ì •ê·œí™”
    # 2) base_share í•©=1 ì •ê·œí™” (+í•©=0ì´ë©´ ê· ë“±ë¶„ë°°)
    sku["base_share"] = pd.to_numeric(sku["base_share"], errors="coerce").fillna(0)

    # í•©>0ì¸ ê·¸ë£¹: 1ë¡œ ì •ê·œí™”
    pos_sum = sku.groupby("cat_low_norm")["base_share"].transform("sum")
    mask_pos = pos_sum > 1e-12
    sku.loc[mask_pos, "base_share"] = sku.loc[mask_pos, "base_share"] / pos_sum[mask_pos]

    # í•©==0ì¸ ê·¸ë£¹: ê· ë“± ë¶„ë°°
    zero_keys = set(sku.loc[~mask_pos, "cat_low_norm"].unique()) - {None, pd.NA, ""}
    if zero_keys:
        for key in zero_keys:
            idx = sku.index[sku["cat_low_norm"] == key]
            n = len(idx)
            if n > 0:
                sku.loc[idx, "base_share"] = 1.0 / n
        print(f"base_share í•© 0 â†’ ê· ë“± ë¶„ë°° ì ìš©: {sorted(list(zero_keys))}")


    # 3) ë‹¤êµ­ì–´ ì´ë¦„ ë³´ì •
    if "sku_name_en" not in sku.columns:
        sku["sku_name_en"] = sku.get("sku_name", sku.get("sku_id"))
    if "sku_name_ko" not in sku.columns:
        sku["sku_name_ko"] = ""

    # 4) ID í˜•ì‹ ê²€ì¦
    _validate_sku_ids(sku)

    # 5) ì¹´í…Œê³ ë¦¬-ì£¼ê°„ ìˆ˜ìš” ì§‘ê³„
    keys = ["warehouse_id", "region", "store_id", "cat_low_norm", "target_date"]
    cat_week = df.groupby(keys, as_index=False)["actual_order_qty"].sum()

    # 6) ì¹´íƒˆë¡œê·¸ ì¡°ì¸ (cat_low_norm ê¸°ì¤€)
    cat_map = cat_week.merge(
        sku.rename(columns={"cat_low_norm": "cat_low_norm_join"}),
        left_on="cat_low_norm", right_on="cat_low_norm_join",
        how="left", validate="many_to_many"
    )
    if cat_map["sku_id"].isna().any():
        missing = cat_map.loc[cat_map["sku_id"].isna(), "cat_low_norm"].unique().tolist()
        raise ValueError(f"sku_catalogì— ì—†ëŠ” cat_lowê°€ ì¡´ì¬í•©ë‹ˆë‹¤(ì •ê·œí™” ê¸°ì¤€): {missing}")

    # 7) share = base_share Ã— (1+noise) Ã— (1+promo)
    np.random.seed(42)
    noise = (np.random.rand(len(cat_map)) - 0.5) * 2 * NOISE_SCALE
    share = cat_map["base_share"] * (1 + noise)

    promo = _load_promo()
    if not promo.empty:
        cat_map = cat_map.merge(promo, on=["sku_id", "target_date"], how="left")
        cat_map["boost"] = cat_map["boost"].fillna(0.0)
        share = share * (1 + cat_map["boost"])

    cat_map["share_adj"] = share
    cat_map["share_norm"] = cat_map.groupby(keys)["share_adj"].transform(_normalize).fillna(0)

    # 8) ë¶„ë°°
    cat_map["sku_qty"] = (cat_map["actual_order_qty"] * cat_map["share_norm"]).round(0).astype(int)

    # 9) ì¶œë ¥
    out_cols = [
        "warehouse_id","region","store_id","target_date",
        # í‘œê¸°ìš© ì¹´í…Œê³ ë¦¬(í‘œì¤€í™”ëœ ê°’ ì‚¬ìš©)
        "cat_low_norm",
        "sku_id","sku_name_en","sku_name_ko","brand","series","model_code",
        "size_inch","volume_l","capacity_text","energy_grade","price_tier",
        "msrp_krw","launch_date","warranty_months","case_pack","min_order_qty","eol_flag",
        "actual_order_qty","share_norm","sku_qty"
    ]
    out = cat_map[out_cols].rename(columns={"cat_low_norm": "cat_low"}) \
                           .sort_values(["store_id","target_date","cat_low","sku_id"]) \
                           .reset_index(drop=True)
    
    cat_label_map = (
    sku.dropna(subset=["cat_low", "cat_low_norm"])
       .drop_duplicates(subset=["cat_low_norm"])
       .set_index("cat_low_norm")["cat_low"]
       .to_dict()
    )
    out["cat_low"] = out["cat_low"].map(cat_label_map).fillna(out["cat_low"])

    OUT_FULL.parent.mkdir(parents=True, exist_ok=True)
    out.to_csv(OUT_FULL, index=False)
    out.sample(min(2000, len(out)), random_state=42).to_csv(OUT_SAMPLE, index=False)

    print(f"saved:\n - {OUT_FULL}\n - {OUT_SAMPLE}\nrows={len(out):,}")

if __name__ == "__main__":
    main()
