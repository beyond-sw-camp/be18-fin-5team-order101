"""
02_expand_sku_from_category.py  (time-varying shares + lifecycle)
- ì…ë ¥: cleaned_domain_sales.csv (ì—†ìœ¼ë©´ domain_sales.csv), sku_catalog.csv
- í•µì‹¬: ì£¼ë³„ ì ìœ ìœ¨ì„ base_share ì£¼ë³€ ëœë¤ì›Œí¬ë¡œ ìƒì„± + ì¶œì‹œ/ë¨í”„ì—…/ê°ì‡  + ë“œë¬¸ í”„ë¡œëª¨ì…˜ ì¶©ê²©
- ì¶œë ¥: domain_sales_sku.csv, sample_domain_sales_sku.csv
"""
from pathlib import Path
import pandas as pd
import numpy as np
import re

BASE = Path(__file__).resolve().parent
DOMAIN_CLEAN = BASE / "cleaned_domain_sales.csv"
DOMAIN_RAW   = BASE / "domain_sales.csv"
SKU_CAT      = BASE / "sku_catalog.csv"
PROMO        = BASE / "promotions.csv"   # optional: sku_id,target_date,boost

OUT_FULL   = BASE / "domain_sales_sku.csv"
OUT_SAMPLE = BASE / "sample_domain_sales_sku.csv"

# ===== Settings =====
NOISE_SCALE = 0.03           # ì£¼ë³„ ë¯¸ì„¸ ì¡ìŒ
RHO = 0.9                    # ì ìœ ìœ¨ ëœë¤ì›Œí¬ ëª¨ë©˜í…€(0.8~0.95)
RW_SIGMA = 0.10              # ëœë¤ì›Œí¬ ë¶„ì‚°(ë¡œê·¸ê³µê°„)
RAMP_WEEKS = 10              # ì¶œì‹œ í›„ ë¨í”„ì—…
DECAY_HALF_LIFE = 90         # ì ˆë°˜ê°ì‡  ì£¼ìˆ˜
PROMO_RATE = 0.05            # ì£¼ë³„ í”„ë¡œëª¨ì…˜ ë°œìƒ í™•ë¥ 
PROMO_STRENGTH = 0.25        # í”„ë¡œëª¨ì…˜ ìŠ¹ìˆ˜
DEFAULT_PROMO_BOOST = 0.20
ID_PATTERN = r"^[A-Z0-9\-]+$"
MISC_KEYS = {"", "ê¸°íƒ€", "ê¸°íƒ€ê°€ì „", "ê¸°íƒ€ê°€ì „ë¥˜", "ê¸°íƒ€ì œí’ˆ", "etc", "misc", "others", "other"}

def _normalize(x):
    s = x.sum()
    return x / s if s > 0 else x

def _norm_cat(x: str) -> str:
    if pd.isna(x): return ""
    x = str(x).strip()
    x = re.sub(r"\s+", "", x)
    x = x.replace("_","").replace("-","")
    return x.lower()

def _load_domain():
    dtypes = {
        "warehouse_id": "Int64","store_id": "Int64",
        "product_id": "Int64","product_name": "string",
        "cat_top": "string","cat_mid": "string","cat_low": "string",
        "region": "string",
    }
    path = DOMAIN_CLEAN if DOMAIN_CLEAN.exists() else DOMAIN_RAW
    df = pd.read_csv(path, parse_dates=["target_date"], dtype=dtypes, low_memory=False)
    print(f"ğŸ“¦ Loaded {path.name}: {len(df):,} rows")
    return df

def _load_catalog():
    sku = pd.read_csv(SKU_CAT, dtype=str, low_memory=False)
    sku.columns = (sku.columns.astype(str).str.replace("\ufeff","",regex=False).str.strip())
    for col in ["sku_id","sku_name_en","sku_name_ko","cat_low","brand","series","model_code",
                "size_inch","volume_l","capacity_text","energy_grade","price_tier",
                "msrp_krw","launch_date","warranty_months","case_pack","min_order_qty","eol_flag","base_share"]:
        if col not in sku.columns:
            sku[col] = pd.NA
    # numeric/date cast
    for col in ["msrp_krw","warranty_months","case_pack","min_order_qty","eol_flag"]:
        sku[col] = pd.to_numeric(sku[col], errors="coerce")
    sku["launch_date"] = pd.to_datetime(sku["launch_date"], errors="coerce")
    sku["base_share"]  = pd.to_numeric(sku["base_share"], errors="coerce").fillna(0.0)
    for col in ["sku_id","sku_name_en","sku_name_ko","cat_low","brand","series","model_code",
                "size_inch","volume_l","capacity_text","energy_grade","price_tier"]:
        sku[col] = sku[col].astype("string").str.strip()
    return sku

def _load_promo():
    if PROMO.exists():
        p = pd.read_csv(PROMO, parse_dates=["target_date"])
        if "boost" not in p.columns: p["boost"] = DEFAULT_PROMO_BOOST
        return p
    return pd.DataFrame(columns=["sku_id","target_date","boost"])

def _validate_sku_ids(sku):
    bad = sku[~sku["sku_id"].astype(str).str.match(ID_PATTERN, na=False)]
    if len(bad) > 0:
        raise ValueError(f"sku_id í˜•ì‹ ì˜¤ë¥˜: ì˜ˆì‹œ {bad.head(5)['sku_id'].tolist()} ... ì „ì²´ {len(bad)}ê±´")

def _weeks_between(a, b):
    return int((b - b.normalize() if isinstance(b, pd.Timestamp) else b - a).days // 7) if pd.notna(a) and pd.notna(b) else 0

def _life_multiplier(launch_date, t):
    """ì¶œì‹œ ì „ 0, ì´í›„ RAMPë¡œ ìƒìŠ¹, ì¥ê¸°ì ìœ¼ë¡œ ì§€ìˆ˜ê°ì‡ """
    if pd.isna(launch_date):
        return 1.0
    weeks = int((t - launch_date).days // 7)
    if weeks < 0:
        return 0.0
    ramp = min(1.0, weeks / max(1, RAMP_WEEKS))
    lam = np.log(2) / max(1, DECAY_HALF_LIFE)
    decay = np.exp(-lam * max(0, weeks - RAMP_WEEKS))
    return ramp * decay + (1 - ramp) * 0.1

def _time_varying_shares(one_cat_week_df, sku_meta, rng):
    """ì¹´í…Œê³ ë¦¬ ì£¼ê°„í•© â†’ SKUë³„ ì‹œê°„ ê°€ë³€ ì ìœ ìœ¨ë¡œ ë¶„ë°°"""
    g = one_cat_week_df.sort_values("target_date").copy()
    sku_rows = sku_meta.reset_index(drop=True)
    n = len(sku_rows)

    base = sku_rows["base_share"].to_numpy().clip(1e-6, 1.0)
    logits = np.log(base)                      # softmax ì´ˆê¸°ê°’
    promos = rng.random((len(g), n)) < PROMO_RATE

    rows = []
    for t_idx, dt in enumerate(g["target_date"]):
        # ëœë¤ì›Œí¬(ëª¨ë©˜í…€+ê°€ìš°ì‹œì•ˆ)
        logits = RHO*logits + (1-RHO)*np.log(base) + rng.normal(0, RW_SIGMA, size=n)
        s = np.exp(logits - np.max(logits)); s /= s.sum()

        # ë¯¸ì„¸ ì¡ìŒ + í”„ë¡œëª¨ì…˜
        s = s * (1 + (rng.random(n)-0.5)*2*NOISE_SCALE)
        if promos[t_idx].any():
            s = s * (1 + promos[t_idx]*PROMO_STRENGTH)

        # ì¶œì‹œ/ìˆ˜ëª… ê³± â†’ ì¬ì •ê·œí™”
        life = np.array([_life_multiplier(sku_rows.loc[i,"launch_date"], dt) for i in range(n)])
        s = s * life
        if s.sum() <= 0:
            s = base.copy()
        s /= s.sum()

        # í–‰ ëˆ„ì 
        rows.append(pd.DataFrame({
            "target_date": dt,
            "sku_id": sku_rows["sku_id"].tolist(),
            "share_norm": s
        }))
    rep = pd.concat(rows, ignore_index=True)
    rep = rep.merge(g[["target_date","actual_order_qty"]], on="target_date", how="left")
    rep["sku_qty"] = (rep["actual_order_qty"] * rep["share_norm"]).round(0).astype(int)
    return rep

def main():
    df  = _load_domain()
    sku = _load_catalog()
    _validate_sku_ids(sku)

    # 1) ì¹´í…Œê³ ë¦¬ í‘œì¤€í™” + ê¸°íƒ€ ì œê±°
    sku["cat_low_norm"] = sku["cat_low"].apply(_norm_cat)
    df["cat_low_norm"]  = df["cat_low"].apply(_norm_cat)
    allowed = set(sku["cat_low_norm"].dropna().tolist())
    keep = df["cat_low_norm"].isin(allowed) & (~df["cat_low_norm"].isin(MISC_KEYS))
    if (len(df) - keep.sum())>0:
        print(f"ğŸ§¹ Dropped by whitelist: {len(df)-keep.sum():,}/{len(df):,}")
    df = df[keep].copy()

    # 2) base_share ì •ê·œí™”(ê·¸ë£¹í•©=1, í•©=0ì´ë©´ ê· ë“±)
    pos_sum = sku.groupby("cat_low_norm")["base_share"].transform("sum")
    mask_pos = pos_sum > 1e-12
    sku.loc[mask_pos, "base_share"] = sku.loc[mask_pos, "base_share"] / pos_sum[mask_pos]
    zero_keys = sku.loc[~mask_pos, "cat_low_norm"].dropna().unique().tolist()
    for key in zero_keys:
        idx = sku.index[sku["cat_low_norm"]==key]
        n = len(idx)
        if n>0: sku.loc[idx,"base_share"] = 1.0/n
    if zero_keys:
        print(f"base_share í•© 0 â†’ ê· ë“±ë¶„ë°°: {sorted(zero_keys)}")

    # 3) ì¹´í…Œê³ ë¦¬-ì£¼ê°„ í•©
    keys = ["warehouse_id","region","store_id","cat_low_norm","target_date"]
    cat_week = df.groupby(keys, as_index=False)["actual_order_qty"].sum()

    # 4) ì¹´íƒˆë¡œê·¸ ì¡°ì¸
    cat_map = cat_week.merge(
        sku.rename(columns={"cat_low_norm":"cat_low_norm_join"}),
        left_on="cat_low_norm", right_on="cat_low_norm_join",
        how="left", validate="many_to_many"
    )
    if cat_map["sku_id"].isna().any():
        missing = cat_map.loc[cat_map["sku_id"].isna(),"cat_low_norm"].unique().tolist()
        raise ValueError(f"ì¹´íƒˆë¡œê·¸ ëˆ„ë½ cat_low_norm: {missing}")

    # 5) ê·¸ë£¹ë³„ ì‹œê°„ê°€ë³€ ì ìœ ìœ¨ ìƒì„±
    rng = np.random.default_rng(42)
    out_list = []
    for (wh, region, store, cat), g_cat in cat_map.groupby(["warehouse_id","region","store_id","cat_low_norm"]):
        sku_meta = (g_cat[["sku_id","base_share","launch_date","sku_name_en","sku_name_ko","brand",
                           "series","model_code","size_inch","volume_l","capacity_text","energy_grade","price_tier",
                           "msrp_krw","warranty_months","case_pack","min_order_qty","eol_flag"]]
                    .drop_duplicates("sku_id"))
        tv = _time_varying_shares(
            one_cat_week_df = g_cat[["target_date","actual_order_qty"]].drop_duplicates(),
            sku_meta = sku_meta, rng = rng
        )
        tv["warehouse_id"] = wh
        tv["region"] = region
        tv["store_id"] = store
        tv["cat_low"] = cat
        tv = tv.merge(sku_meta, on="sku_id", how="left")
        out_list.append(tv)

    out = pd.concat(out_list, ignore_index=True)
    out = out[[
        "warehouse_id","region","store_id","target_date","cat_low",
        "sku_id","sku_name_en","sku_name_ko","brand","series","model_code",
        "size_inch","volume_l","capacity_text","energy_grade","price_tier",
        "msrp_krw","launch_date","warranty_months","case_pack","min_order_qty","eol_flag",
        "actual_order_qty","share_norm","sku_qty"
    ]].sort_values(["store_id","target_date","cat_low","sku_id"]).reset_index(drop=True)

    OUT_FULL.parent.mkdir(parents=True, exist_ok=True)
    out.to_csv(OUT_FULL, index=False)
    out.sample(min(2000,len(out)), random_state=42).to_csv(OUT_SAMPLE, index=False)
    print(f"saved:\n - {OUT_FULL}\n - {OUT_SAMPLE}\nrows={len(out):,}")

if __name__ == "__main__":
    main()
